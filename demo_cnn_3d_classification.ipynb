{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN to classify large 3D regions\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* Keras (with TensorFlow backend)\n",
    "* nibabel (interface to nifti files)\n",
    "* scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution3D, MaxPooling3D, Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.regularizers import l2\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to load 3D images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# load image\n",
    "################################################\n",
    "\n",
    "def img_load(load_data_flag, img_suffix, dir_class1, dir_class2, dir_all, idno, train_ind, train_train_ind, train_val_ind, test_ind):\n",
    "\n",
    "    if load_data_flag:\n",
    "\n",
    "        # loading all the image together does not work for large dataset (due to the limit in memory)\n",
    "\n",
    "        img_train_class1 = np.array(\n",
    "            [nib.load(dir_class1+str(id)+img_suffix).get_data() for id in\n",
    "             [idno[i] for i in train_ind[train_train_ind]]])\n",
    "        img_train_class2 = np.array(\n",
    "            [nib.load(dir_class2+str(id)+img_suffix).get_data() for id in\n",
    "             [idno[i] for i in train_ind[train_train_ind]]])\n",
    "\n",
    "        img_val_class1 = np.array(\n",
    "            [nib.load(dir_class1+str(id)+img_suffix).get_data() for id in\n",
    "             [idno[i] for i in train_ind[train_val_ind]]])\n",
    "        img_val_class2 = np.array(\n",
    "            [nib.load(dir_class2+str(id)+img_suffix).get_data() for id in\n",
    "             [idno[i] for i in train_ind[train_val_ind]]])\n",
    "\n",
    "        img_test_class1 = np.array(\n",
    "            [nib.load(dir_class1 + str(id)+img_suffix).get_data() for id in\n",
    "            [idno[i] for i in test_ind]])\n",
    "        img_test_class2 = np.array(\n",
    "            [nib.load(dir_class2 + str(id)+img_suffix).get_data() for id in\n",
    "            [idno[i] for i in test_ind]])\n",
    "\n",
    "        img_train = \\\n",
    "        np.zeros((img_train_class1.shape[0]*2,img_train_class1.shape[1],img_train_class1.shape[2],img_train_class1.shape[3]))\n",
    "        img_train[range(0,img_train.shape[0],2),:,:,:]=img_train_class1\n",
    "        img_train[(range(1,img_train.shape[0],2)),:,:,:]=img_train_class2\n",
    "        img_val  = np.concatenate((img_val_class1,img_val_class2),axis=0)      \n",
    "        img_test = np.concatenate((img_test_class1,img_test_class2),axis=0)\n",
    "\n",
    "        img_train = np.expand_dims(img_train,axis=1)\n",
    "        img_val   = np.expand_dims(img_val, axis=1)\n",
    "        img_test  = np.expand_dims(img_test,axis=1)\n",
    "\n",
    "        label_train = np.zeros(img_train.shape[0])\n",
    "        label_train[range(0,img_train.shape[0],2)]=0\n",
    "        label_train[range(1,img_train.shape[0],2)]=1        \n",
    "        label_val  = np.concatenate((np.zeros(img_val.shape[0]/2),np.ones(img_val.shape[0]/2)),axis=0)\n",
    "        label_test = np.concatenate((np.zeros(img_test.shape[0]/2),np.ones(img_test.shape[0]/2)),axis=0)\n",
    "\n",
    "        np.save(dir_all+'img_train.npy',img_train)\n",
    "        np.save(dir_all+'img_val.npy', img_val)\n",
    "        np.save(dir_all+'img_test.npy',img_test)\n",
    "        np.save(dir_all+'label_train.npy',label_train)\n",
    "        np.save(dir_all+'label_val.npy', label_val)\n",
    "        np.save(dir_all+'label_test.npy',label_test)\n",
    "        \n",
    "    else:\n",
    "        img_train   = np.load(dir_all + 'img_train.npy')\n",
    "        img_val     = np.load(dir_all+'img_val.npy')\n",
    "        img_test    = np.load(dir_all+'img_test.npy')\n",
    "        label_train = np.load(dir_all + 'label_train.npy')\n",
    "        label_val   = np.load(dir_all+'label_val.npy')\n",
    "        label_test  = np.load(dir_all+'label_test.npy')\n",
    "       \n",
    "    \n",
    "    return img_train, img_val, img_test, label_train, label_val, label_test\n",
    " \n",
    "################################################\n",
    "# image normalization\n",
    "################################################\n",
    "\n",
    "def img_norm(img, int_max, int_min, int_mean):\n",
    "    \n",
    "    img[img < int_min]    = int_min\n",
    "    img[img > int_max]    = int_max\n",
    "    img = (img - int_mean)*1.0/(int_max - int_min)\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(img_train, img_val, label_train, label_val, image_size, depth_level):\n",
    "    model = get_network(image_size, depth_level)\n",
    "    model_checkpoint = ModelCheckpoint('model.hdf5', monitor='loss', save_best_only=True)\n",
    "    model.summary()\n",
    "    model.fit(img_train, label_train, batch_size=8, epochs=10, verbose=1, shuffle=False,\n",
    "              validation_data=[img_val,label_val])    \n",
    "    return model\n",
    "\n",
    "def get_network(img_size, depth_level):\n",
    "    inputs = Input((1,img_size[0], img_size[1], img_size[2]))\n",
    "    first_channel_num = 4\n",
    "    conv1 = Convolution3D(first_channel_num, (3, 3, 3), activation='relu', padding = 'same',\n",
    "                          kernel_regularizer=l2(0.00))(inputs)\n",
    "    conv1 = Convolution3D(first_channel_num, (3, 3, 3), activation=None, padding = 'same',\n",
    "                          kernel_regularizer=l2(0.00))(conv1)\n",
    "    bn1 = BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                       gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones',\n",
    "                       beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)(conv1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(relu1)\n",
    "\n",
    "    conv2 = Convolution3D(first_channel_num*2, (3, 3, 3), activation='relu', padding = 'same',\n",
    "                          kernel_regularizer=l2(0.00))(pool1)\n",
    "    conv2 = Convolution3D(first_channel_num*2, (3, 3, 3), activation=None, padding = 'same',\n",
    "                          kernel_regularizer=l2(0.00))(conv2)\n",
    "    bn2 = BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                             gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                             moving_variance_initializer='ones',\n",
    "                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,\n",
    "                             gamma_constraint=None)(conv2)\n",
    "    relu2 = Activation('relu')(bn2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(relu2)\n",
    "\n",
    "    conv3 = Convolution3D(first_channel_num*4, (3, 3, 3), activation='relu', padding = 'same',\n",
    "                          kernel_regularizer=l2(0.00))(pool2)\n",
    "    conv3 = Convolution3D(first_channel_num * 4, (3, 3, 3), activation=None, padding='same',\n",
    "                          kernel_regularizer=l2(0.00))(conv3)\n",
    "    bn3 = BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                             gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                             moving_variance_initializer='ones',\n",
    "                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,\n",
    "                             gamma_constraint=None)(conv3)\n",
    "    relu3 = Activation('relu')(bn3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(relu3)\n",
    "\n",
    "    conv4 = Convolution3D(first_channel_num*8, (3, 3, 3), activation='relu', padding = 'same',\n",
    "                          kernel_regularizer=l2(0.00))(pool3)\n",
    "    conv4 = Convolution3D(first_channel_num * 8, (3, 3, 3), activation=None, padding='same',\n",
    "                          kernel_regularizer=l2(0.00))(conv4)\n",
    "    bn4 = BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                             gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                             moving_variance_initializer='ones',\n",
    "                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,\n",
    "                             gamma_constraint=None)(conv4)\n",
    "    relu4 = Activation('relu')(bn4)\n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(relu4)\n",
    "\n",
    "    conv5 = Convolution3D(first_channel_num * 16, (3, 3, 3), activation='relu', padding='same',\n",
    "                          kernel_regularizer=l2(0.00))(pool4)\n",
    "    conv5 = Convolution3D(first_channel_num * 16, (3, 3, 3), activation=None, padding='same',\n",
    "                          kernel_regularizer=l2(0.00))(conv5)\n",
    "    bn5 = BatchNormalization(axis=1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                             gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                             moving_variance_initializer='ones',\n",
    "                             beta_regularizer=None, gamma_regularizer=None, beta_constraint=None,\n",
    "                             gamma_constraint=None)(conv5)\n",
    "    relu5 = Activation('relu')(bn5)\n",
    "    pool5 = MaxPooling3D(pool_size=(2, 2, 2), strides=2)(relu5)\n",
    "    \n",
    "    if depth_level == 1:\n",
    "        flatten1 = Flatten()(pool1)\n",
    "    elif depth_level == 2:\n",
    "        flatten1 = Flatten()(pool2)\n",
    "    elif depth_level == 3:\n",
    "        flatten1 = Flatten()(pool3)\n",
    "    elif depth_level == 4:\n",
    "        flatten1 = Flatten()(pool4)\n",
    "    else:\n",
    "        flatten1 = Flatten()(pool5)\n",
    "\n",
    "    fc1 = Dense(1, activation='sigmoid', use_bias=True)(flatten1)\n",
    "\n",
    "    model = Model(output=fc1,input=inputs)\n",
    "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# Predefined parameters\n",
    "####################################################################\n",
    "\n",
    "img_size       = [128, 128, 128] # size of ROI\n",
    "int_min        = -1000  # min value for intensity normalization\n",
    "int_max        = 0      # max value for intensity normalization\n",
    "int_mean       = -800   # mean value for intensity normalization\n",
    "load_data_flag = True\n",
    "\n",
    "####################################################################\n",
    "# Predefined directories\n",
    "####################################################################\n",
    "\n",
    "dir_class1  = '../sample_class1/'\n",
    "dir_class2  = '../sample_class2/'\n",
    "dir_all     = '../sample_all/'\n",
    "\n",
    "img_suffix  = '.nii.gz' # suffix for 3D data\n",
    "idno_list   = sorted(glob(dir_class1+'*'+img_suffix))\n",
    "idno        = []\n",
    "\n",
    "for id in range(0,len(idno_list)):\n",
    "    idno.append(idno_list[id].split('/')[-1][:-len(img_suffix)])\n",
    "\n",
    "####################################################################\n",
    "# Split training, validation and test\n",
    "####################################################################\n",
    "\n",
    "# train vs. val vs. test = 4:1:1\n",
    "skf_train_test = StratifiedKFold(n_splits=6,random_state=1)\n",
    "skf_train_val  = StratifiedKFold(n_splits=5,random_state=1)\n",
    "\n",
    "# change stratify_group if you have prior knowledge (e.g. demographic info) for stratified sampling\n",
    "stratify_group = np.ones(len(idno))\n",
    "\n",
    "####################################################################\n",
    "# Begin training and evaluation\n",
    "####################################################################\n",
    "\n",
    "for train_ind,test_ind in skf_train_test.split(idno,stratify_group):\n",
    "    \n",
    "    # just access the first fold without a for loop\n",
    "    train_train_ind, train_val_ind = list(skf_train_val.split([idno[i] for i in train_ind], stratify_group[train_ind]))[0]\n",
    "      \n",
    "    img_train, img_val, img_test, label_train, label_val, label_test = \\\n",
    "    img_load(load_data_flag, img_suffix, dir_class1, dir_class2, dir_all, idno, train_ind, train_train_ind, train_val_ind, test_ind)\n",
    "    \n",
    "    img_train = img_norm(img_train, int_max, int_min, int_mean)\n",
    "    img_val   = img_norm(img_val, int_max, int_min, int_mean)\n",
    "    img_test  = img_norm(img_test, int_max, int_min, int_mean)\n",
    "        \n",
    "    depth_level  = 4 # number of conv layers = depth_level * 2\n",
    "    model        = train_network(img_train,img_val,label_train,label_val,img_size,depth_level)\n",
    "    test_predict = model.predict(img_test)\n",
    "    model.evaluate(img_test,label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
